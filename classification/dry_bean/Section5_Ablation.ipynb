{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Section 5: Ablation Studies\n",
      "Dataset: Dry Bean Classification\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom analysis frameworks\n",
    "from explainability_analysis import ExplainabilityAnalyzer, clear_memory, save_intermediate_results,load_intermediate_results\n",
    "from enhanced_evaluation import ComprehensiveEvaluator\n",
    "from ablation_studies import AblationStudyAnalyzer\n",
    "from enhanced_ablation_studies import run_enhanced_ablation_studies, load_model_ablation_results\n",
    "\n",
    "# Memory management utilities\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import logging\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure logging to file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('Ablation.log'),\n",
    "        logging.StreamHandler(sys.stdout)  # Also print to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Usage\n",
    "logging.info(\"This will be written to Ablation.log\")\n",
    "logging.error(\"Error messages too\")\n",
    "\n",
    "print(\"üîç Section 5: Ablation Studies\")\n",
    "print(\"Dataset: Dry Bean Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Section 2 results loaded successfully!\n",
      "Models available: ['XGBoost', 'TabPFN v2', 'TabICL', 'FT-Transformer']\n",
      "Features: 16\n",
      "Classes: 7\n",
      "Test samples: 2,723\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('Section5_Ablation', exist_ok=True)\n",
    "\n",
    "# Load trained models and results from Section 2\n",
    "try:\n",
    "    with open('dry_bean_section2_results.pkl', 'rb') as f:\n",
    "        section2_data = pickle.load(f)\n",
    "\n",
    "    # Extract variables\n",
    "    models = section2_data['models']\n",
    "    evaluator = section2_data['evaluator']\n",
    "    X_train_scaled = section2_data['X_train_scaled']\n",
    "    X_val_scaled = section2_data['X_val_scaled']\n",
    "    X_test_scaled = section2_data['X_test_scaled']\n",
    "    y_train = section2_data['y_train']\n",
    "    y_val = section2_data['y_val']\n",
    "    y_test = section2_data['y_test']\n",
    "    feature_names = section2_data['feature_names']\n",
    "    class_mapping = section2_data['class_mapping']\n",
    "    class_names = section2_data['class_names']\n",
    "    label_encoder = section2_data['label_encoder']\n",
    "    comparison_df = section2_data['comparison_df']\n",
    "\n",
    "    print(\"‚úÖ Section 2 results loaded successfully!\")\n",
    "    print(f\"Models available: {list(models.keys())}\")\n",
    "    print(f\"Features: {len(feature_names)}\")\n",
    "    print(f\"Classes: {len(class_names)}\")\n",
    "    print(f\"Test samples: {len(X_test_scaled):,}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Section 2 results not found!\")\n",
    "    print(\"Please run Section 2 (Model Training) notebook first.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "üîß Creating PyTorch wrapper for FT-Transformer...\n",
      "‚úÖ FT-Transformer wrapper created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create FT-Transformer PyTorch wrapper for ablation studies\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check if FT-Transformer exists in models\n",
    "if 'FT-Transformer' in models:\n",
    "    print(\"üîß Creating PyTorch wrapper for FT-Transformer...\")\n",
    "    original_ft_transformer = models['FT-Transformer']\n",
    "    \n",
    "    try:\n",
    "        ft_wrapper = evaluator.create_pytorch_wrapper(\n",
    "            model=original_ft_transformer,\n",
    "            device=device,\n",
    "            batch_size=256\n",
    "        )\n",
    "        \n",
    "        # Replace the original model with the wrapper for ablation studies\n",
    "        models['FT-Transformer'] = ft_wrapper\n",
    "        print(\"‚úÖ FT-Transformer wrapper created successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Failed to create FT-Transformer wrapper: {e}\")\n",
    "        print(\"Continuing with original FT-Transformer model...\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è FT-Transformer not found in models, skipping wrapper creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Advanced XGBoost, FT-transformer, TabICL and TabPFN Ablation Analysis\n",
    "\n",
    "This notebook integrates enhanced ablation studies for TabICL and TabPFN models, building upon the comprehensive comparison analysis. It assumes that the following are already available from the comprehensive comparison notebook:\n",
    "\n",
    "- **Preprocessed Data**: `X_train_scaled`, `X_test_scaled`, `y_train`, `y_test`, `feature_names`\n",
    "- **Trained Models**: `models` dictionary containing XGBoost, TabPFN v2, and TabICL\n",
    "- **Model Names**: `model_names` list\n",
    "\n",
    "### Enhanced Ablation Studies Include:\n",
    "- **TabPFN-Specific**: Context size optimization, device performance, memory efficiency\n",
    "- **TabICL-Specific**: In-context learning examples, example selection strategies, context window utilization\n",
    "- **Cross-Model Analysis**: Feature importance comparison, robustness analysis\n",
    "- **Production Insights**: Performance recommendations and deployment considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Enhanced Ablation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Enhanced Ablation Studies Framework Loaded\n",
      "Ready to perform advanced ablation analysis on TabICL and TabPFN models\n"
     ]
    }
   ],
   "source": [
    "# Import enhanced ablation studies framework\n",
    "from enhanced_ablation_studies import (\n",
    "    EnhancedAblationStudyAnalyzer, \n",
    "    run_enhanced_ablation_studies,\n",
    "    create_ablation_summary_dataframe,\n",
    "    plot_ablation_dashboard\n",
    ")\n",
    "\n",
    "print(\"üî¨ Enhanced Ablation Studies Framework Loaded\")\n",
    "print(\"Ready to perform advanced ablation analysis on TabICL and TabPFN models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Available Resources\n",
    "\n",
    "Let's confirm that all required resources from the comprehensive comparison are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying Required Resources:\n",
      "   ‚úÖ X_train_scaled: shape (8710, 16)\n",
      "   ‚úÖ X_test_scaled: shape (2723, 16)\n",
      "   ‚úÖ y_train: length 8710\n",
      "   ‚úÖ y_test: length 2723\n",
      "   ‚úÖ feature_names: 16 features\n",
      "   ‚úÖ models: ['XGBoost', 'TabPFN v2', 'TabICL', 'FT-Transformer']\n",
      "   ‚úÖ model_names: available\n",
      "\n",
      "üìä Models available for enhanced ablation studies: dict_keys(['XGBoost', 'TabPFN v2', 'TabICL', 'FT-Transformer'])\n"
     ]
    }
   ],
   "source": [
    "# Verify that required variables are available from comprehensive comparison notebook\n",
    "required_vars = [\n",
    "    'X_train_scaled', 'X_test_scaled', 'y_train', 'y_test', \n",
    "    'feature_names', 'models', 'model_names'\n",
    "]\n",
    "\n",
    "model_names= models.keys()\n",
    "import os\n",
    "os.environ['TABPFN_ALLOW_CPU_LARGE_DATASET'] = '1'\n",
    "print(\"üîç Verifying Required Resources:\")\n",
    "for var_name in required_vars:\n",
    "    if var_name in globals():\n",
    "        if var_name == 'models':\n",
    "            print(f\"   ‚úÖ {var_name}: {list(models.keys())}\")\n",
    "        elif var_name in ['X_train_scaled', 'X_test_scaled']:\n",
    "            print(f\"   ‚úÖ {var_name}: shape {globals()[var_name].shape}\")\n",
    "        elif var_name in ['y_train', 'y_test']:\n",
    "            print(f\"   ‚úÖ {var_name}: length {len(globals()[var_name])}\")\n",
    "        elif var_name == 'feature_names':\n",
    "            print(f\"   ‚úÖ {var_name}: {len(feature_names)} features\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ {var_name}: available\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {var_name}: NOT FOUND - Please run comprehensive comparison notebook first\")\n",
    "\n",
    "print(f\"\\nüìä Models available for enhanced ablation studies: {model_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Enhanced Ablation Studies\n",
    "\n",
    "Now we'll run the comprehensive enhanced ablation studies specifically designed for TabICL and TabPFN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize enhanced ablation study analyzer\n",
    "print(\"üöÄ INITIALIZING ENHANCED ABLATION STUDIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "enhanced_analyzer = EnhancedAblationStudyAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Run comprehensive ablation studies\n",
    "print(\"\\nüî¨ Starting enhanced ablation analysis...\")\n",
    "print(\"This will perform model-specific ablations for TabICL and TabPFN\")\n",
    "print(\"Expected duration: 5-15 minutes depending on hardware\")\n",
    "\n",
    "enhanced_ablation_results = enhanced_analyzer.comprehensive_ablation_study(\n",
    "    models_dict=models,\n",
    "    model_names=model_names,\n",
    "    X_train=X_train_scaled,\n",
    "    X_test=X_test_scaled,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Ablation Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "ablation_summary_df = create_ablation_summary_dataframe(enhanced_ablation_results)\n",
    "\n",
    "print(\"\\nüìä ENHANCED ABLATION STUDY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(ablation_summary_df.round(4).to_string())\n",
    "\n",
    "# Save results for further analysis\n",
    "ablation_summary_df.to_csv('enhanced_ablation_summary.csv', index=False)\n",
    "print(\"\\nüíæ Results saved to 'enhanced_ablation_summary.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Ablation Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp = load_model_ablation_results('XGBoost')\n",
    "\n",
    "#enhanced_analyzer.results[temp['model_name']] = temp['results']\n",
    "\n",
    "#temp = load_model_ablation_results('FT-Transformer')\n",
    "\n",
    "#enhanced_analyzer.results[temp['model_name']] = temp['results']\n",
    "\n",
    "#temp = load_model_ablation_results('TabICL')\n",
    "\n",
    "#enhanced_analyzer.results[temp['model_name']] = temp['results']\n",
    "\n",
    "#temp = load_model_ablation_results('TabPFN v2')\n",
    "\n",
    "#enhanced_analyzer.results[temp['model_name']] = temp['results']\n",
    "#print(enhanced_analyzer.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# enhanced_analyzer.results['XGBoost']\n",
    "# Generate comprehensive dashboard\n",
    "print(\"üìà Generating Enhanced Ablation Dashboard...\")\n",
    "plot_ablation_dashboard(enhanced_analyzer, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabPFN-Specific Enhanced Ablations\n",
    "\n",
    "Deep dive into TabPFN-specific ablation studies including context size optimization, device performance analysis, and memory efficiency testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabPFN-specific analysis\n",
    "# enhanced_ablation_results = enhanced_analyzer.results\n",
    "if 'TabPFN v2' in enhanced_ablation_results:\n",
    "    tabpfn_results = enhanced_ablation_results['TabPFN v2']\n",
    "    \n",
    "    print(\"\\nüî¨ TABPFN ENHANCED ABLATION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Context Size Optimization\n",
    "    if 'context_size_ablation' in tabpfn_results:\n",
    "        context_results = tabpfn_results['context_size_ablation']\n",
    "        \n",
    "        print(\"\\nüìè Context Size Optimization Results:\")\n",
    "        context_df = pd.DataFrame(context_results)\n",
    "        print(context_df.round(4).to_string())\n",
    "        \n",
    "        # Visualization\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Performance vs Context Size\n",
    "        ax1.plot(context_df['context_size'], context_df['f1_score'], 'o-', linewidth=2, markersize=8, color='blue')\n",
    "        ax1.set_xlabel('Context Size')\n",
    "        ax1.set_ylabel('F1 Score')\n",
    "        ax1.set_title('TabPFN: Context Size vs Performance')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training Time vs Context Size\n",
    "        ax2.plot(context_df['context_size'], context_df['train_time'], 'o-', linewidth=2, markersize=8, color='red')\n",
    "        ax2.set_xlabel('Context Size')\n",
    "        ax2.set_ylabel('Training Time (seconds)')\n",
    "        ax2.set_title('TabPFN: Context Size vs Training Time')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy vs Context Size\n",
    "        ax3.plot(context_df['context_size'], context_df['accuracy'], 'o-', linewidth=2, markersize=8, color='green')\n",
    "        ax3.set_xlabel('Context Size')\n",
    "        ax3.set_ylabel('Accuracy')\n",
    "        ax3.set_title('TabPFN: Context Size vs Accuracy')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Efficiency (Performance/Time)\n",
    "        efficiency = context_df['f1_score'] / context_df['train_time']\n",
    "        ax4.plot(context_df['context_size'], efficiency, 'o-', linewidth=2, markersize=8, color='purple')\n",
    "        ax4.set_xlabel('Context Size')\n",
    "        ax4.set_ylabel('Efficiency (F1/Time)')\n",
    "        ax4.set_title('TabPFN: Context Size vs Efficiency')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig('TabPFN_Ablation_Results.png')\n",
    "        \n",
    "        # Optimal context size analysis\n",
    "        best_f1_idx = context_df['f1_score'].idxmax()\n",
    "        best_efficiency_idx = efficiency.idxmax()\n",
    "        \n",
    "        print(f\"\\nüéØ TabPFN Context Size Insights:\")\n",
    "        print(f\"   Optimal for F1: Context size {context_df.loc[best_f1_idx, 'context_size']} (F1: {context_df.loc[best_f1_idx, 'f1_score']:.4f})\")\n",
    "        print(f\"   Optimal for Efficiency: Context size {context_df.loc[best_efficiency_idx, 'context_size']} (Efficiency: {efficiency.iloc[best_efficiency_idx]:.4f})\")\n",
    "        print(f\"   Performance Range: {context_df['f1_score'].min():.4f} - {context_df['f1_score'].max():.4f}\")\n",
    "        print(f\"   Time Range: {context_df['train_time'].min():.2f}s - {context_df['train_time'].max():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabICL-Specific Enhanced Ablations\n",
    "\n",
    "Comprehensive analysis of TabICL's in-context learning capabilities, including context examples optimization, example selection strategies, and context window utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabICL-specific analysis\n",
    "if 'TabICL' in enhanced_ablation_results:\n",
    "    tabicl_results = enhanced_ablation_results['TabICL']\n",
    "    \n",
    "    print(\"\\nüéØ TABICL ENHANCED ABLATION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Context Examples Optimization\n",
    "    if 'context_examples_ablation' in tabicl_results:\n",
    "        examples_results = tabicl_results['context_examples_ablation']\n",
    "        \n",
    "        print(\"\\nüìù Context Examples Optimization Results:\")\n",
    "        examples_df = pd.DataFrame(examples_results)\n",
    "        print(examples_df.round(4).to_string())\n",
    "        \n",
    "        # Visualization\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Performance vs Context Examples\n",
    "        ax1.plot(examples_df['context_examples'], examples_df['f1_score'], 'o-', \n",
    "                linewidth=2, markersize=8, color='purple')\n",
    "        ax1.set_xlabel('Number of Context Examples')\n",
    "        ax1.set_ylabel('F1 Score')\n",
    "        ax1.set_title('TabICL: Context Examples vs Performance')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training Time vs Context Examples\n",
    "        ax2.plot(examples_df['context_examples'], examples_df['train_time'], 'o-', \n",
    "                linewidth=2, markersize=8, color='red')\n",
    "        ax2.set_xlabel('Number of Context Examples')\n",
    "        ax2.set_ylabel('Training Time (seconds)')\n",
    "        ax2.set_title('TabICL: Context Examples vs Training Time')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy vs Context Examples\n",
    "        ax3.plot(examples_df['context_examples'], examples_df['accuracy'], 'o-', \n",
    "                linewidth=2, markersize=8, color='green')\n",
    "        ax3.set_xlabel('Number of Context Examples')\n",
    "        ax3.set_ylabel('Accuracy')\n",
    "        ax3.set_title('TabICL: Context Examples vs Accuracy')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning Efficiency\n",
    "        learning_efficiency = examples_df['f1_score'] / examples_df['context_examples']\n",
    "        ax4.plot(examples_df['context_examples'], learning_efficiency, 'o-', \n",
    "                linewidth=2, markersize=8, color='orange')\n",
    "        ax4.set_xlabel('Number of Context Examples')\n",
    "        ax4.set_ylabel('Learning Efficiency (F1/Examples)')\n",
    "        ax4.set_title('TabICL: Context Examples vs Learning Efficiency')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig('TabICL_Ablation_Results.png')\n",
    "        \n",
    "        # Optimal context examples analysis\n",
    "        best_f1_idx = examples_df['f1_score'].idxmax()\n",
    "        best_efficiency_idx = learning_efficiency.idxmax()\n",
    "        \n",
    "        print(f\"\\nüéØ TabICL Context Examples Insights:\")\n",
    "        print(f\"   Optimal for F1: {examples_df.loc[best_f1_idx, 'context_examples']} examples (F1: {examples_df.loc[best_f1_idx, 'f1_score']:.4f})\")\n",
    "        print(f\"   Most Efficient: {examples_df.loc[best_efficiency_idx, 'context_examples']} examples (Efficiency: {learning_efficiency.iloc[best_efficiency_idx]:.4f})\")\n",
    "        print(f\"   Performance Range: {examples_df['f1_score'].min():.4f} - {examples_df['f1_score'].max():.4f}\")\n",
    "        print(f\"   Time Range: {examples_df['train_time'].min():.2f}s - {examples_df['train_time'].max():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Model Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-model feature importance comparison\n",
    "#temp = {}\n",
    "#filename='online_shoppers_section5_ablation_results.pkl'\n",
    "#with open(filename, 'rb') as f:\n",
    "#        temp = pickle.load(f)\n",
    "\n",
    "#print(temp)\n",
    "#enhanced_ablation_results = temp['all_results']\n",
    "if 'comparative_analysis' in enhanced_ablation_results:\n",
    "    comparative_results = enhanced_ablation_results['comparative_analysis']\n",
    "    \n",
    "    print(\"\\nüéØ CROSS-MODEL FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if 'feature_importance_comparison' in comparative_results:\n",
    "        importance_comparison = comparative_results['feature_importance_comparison']\n",
    "        \n",
    "        # Create feature importance comparison table\n",
    "        all_features = set()\n",
    "        for model_importance in importance_comparison.values():\n",
    "            all_features.update(model_importance.keys())\n",
    "        all_features = sorted(list(all_features))\n",
    "        \n",
    "        importance_df = pd.DataFrame(index=all_features)\n",
    "        for model_name, feature_importance in importance_comparison.items():\n",
    "            importance_df[model_name] = [feature_importance.get(feature, 0) for feature in all_features]\n",
    "        \n",
    "        print(\"\\nFeature Importance Comparison (Relative Importance):\")\n",
    "        print(importance_df.round(4).to_string())\n",
    "        \n",
    "        # Plot feature importance heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(importance_df.T, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "                   cbar_kws={'label': 'Relative Importance'})\n",
    "        plt.title('Feature Importance Comparison Across Models')\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Models')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig('Ablation_Feature_Imp_Across_Models_Results.png')\n",
    "        \n",
    "        # Top features consensus\n",
    "        print(\"\\nüèÜ TOP FEATURES CONSENSUS:\")\n",
    "        \n",
    "        # Calculate average importance across models\n",
    "        avg_importance = importance_df.mean(axis=1).sort_values(ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 5 Most Important Features (Average Across Models):\")\n",
    "        for i, (feature, importance) in enumerate(avg_importance.head().items()):\n",
    "            print(f\"   {i+1}. {feature}: {importance:.4f}\")\n",
    "        \n",
    "        # Model agreement analysis\n",
    "        print(\"\\nModel Agreement on Top Features:\")\n",
    "        for model in importance_df.columns:\n",
    "            top_features = importance_df[model].nlargest(3).index.tolist()\n",
    "            print(f\"   {model}: {', '.join(top_features)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
