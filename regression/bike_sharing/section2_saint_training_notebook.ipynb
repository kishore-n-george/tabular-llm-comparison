{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAINT Training for Bike Sharing Regression\n",
    "\n",
    "This notebook demonstrates the training and evaluation of a **SAINT (Self-Attention and Intersample Attention Transformer)** model for bike sharing demand prediction.\n",
    "\n",
    "## SAINT Architecture Overview\n",
    "\n",
    "SAINT is a transformer-based architecture specifically designed for tabular data that combines:\n",
    "- **Self-attention mechanisms** to capture feature interactions within samples\n",
    "- **Intersample attention** to learn patterns across different samples in a batch\n",
    "- **Feature embeddings** for numerical features\n",
    "- **Positional encoding** to maintain feature order information\n",
    "\n",
    "## Key Features\n",
    "- Comprehensive logging to file\n",
    "- Model checkpointing (both .pth and .pkl formats)\n",
    "- Detailed training and evaluation plots\n",
    "- Performance metrics tracking\n",
    "- Early stopping with patience\n",
    "- Learning rate scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SAINT training functions\n",
    "try:\n",
    "    from saint_training_functions import *\n",
    "    print(\"‚úÖ SAINT training functions imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(\"‚ùå SAINT training functions not available.\")\n",
    "    print(\"Please ensure saint_training_functions.py is in the same directory.\")\n",
    "    print(f\"Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device and configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    'data_path': './bike_sharing_preprocessed_data.pkl',\n",
    "    'device': device,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'd_model': 128,\n",
    "    'n_heads': 8,\n",
    "    'n_layers': 6,\n",
    "    'save_dir': './Section2_Model_Training'\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "print(\"üìä Loading and preparing data...\")\n",
    "\n",
    "(X_train_scaled, X_val_scaled, X_test_scaled, \n",
    " y_train, y_val, y_test, feature_names, data_summary) = load_preprocessed_data(config['data_path'])\n",
    "\n",
    "print(f\"\\nüìà Data Summary:\")\n",
    "print(f\"   Training samples: {X_train_scaled.shape[0]:,}\")\n",
    "print(f\"   Validation samples: {X_val_scaled.shape[0]:,}\")\n",
    "print(f\"   Test samples: {X_test_scaled.shape[0]:,}\")\n",
    "print(f\"   Features: {len(feature_names)}\")\n",
    "print(f\"   Target range: [{y_train.min():.0f}, {y_train.max():.0f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature names\n",
    "print(\"üè∑Ô∏è Feature Names:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"   {i+1:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "(train_loader, val_loader, test_loader,\n",
    " X_train_tensor, X_val_tensor, X_test_tensor,\n",
    " y_train_tensor, y_val_tensor, y_test_tensor) = prepare_data_for_training(\n",
    "    X_train_scaled, X_val_scaled, X_test_scaled, \n",
    "    y_train, y_val, y_test, feature_names, \n",
    "    config['device'], config['batch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Creation and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SAINT model\n",
    "model, total_params = create_saint_model(\n",
    "    n_features=len(feature_names),\n",
    "    device=config['device'],\n",
    "    d_model=config['d_model'],\n",
    "    n_heads=config['n_heads'],\n",
    "    n_layers=config['n_layers']\n",
    ")\n",
    "\n",
    "print(f\"\\nüèóÔ∏è SAINT Model Architecture:\")\n",
    "print(f\"   Input features: {len(feature_names)}\")\n",
    "print(f\"   Model dimension: {config['d_model']}\")\n",
    "print(f\"   Attention heads: {config['n_heads']}\")\n",
    "print(f\"   Transformer layers: {config['n_layers']}\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training components\n",
    "criterion, optimizer, scheduler, training_config = setup_training(\n",
    "    model, \n",
    "    learning_rate=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "print(f\"\\nüîß Training Configuration:\")\n",
    "print(f\"   Learning rate: {training_config['learning_rate']}\")\n",
    "print(f\"   Weight decay: {training_config['weight_decay']}\")\n",
    "print(f\"   Max epochs: {training_config['n_epochs']}\")\n",
    "print(f\"   Early stopping patience: {training_config['patience']}\")\n",
    "print(f\"   Loss function: MSE (regression)\")\n",
    "print(f\"   Optimizer: AdamW\")\n",
    "print(f\"   Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logger = setup_logging(config['save_dir'])\n",
    "logger.info(\"Starting SAINT training from notebook\")\n",
    "\n",
    "print(\"üöÄ Starting SAINT model training...\")\n",
    "print(\"üìù Training progress will be logged to file and displayed here.\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model, history, best_epoch, training_time = train_saint_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "    training_config, config['device'], logger\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"   Best epoch: {best_epoch + 1}\")\n",
    "print(f\"   Training time: {training_time:.2f} seconds\")\n",
    "print(f\"   Final validation R¬≤: {history['val_r2'][best_epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "predictions, metrics = evaluate_model(\n",
    "    model, X_test_tensor, y_test_tensor, config['device'], logger\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Final Test Performance:\")\n",
    "print(f\"   R¬≤ Score: {metrics['r2_score']:.4f}\")\n",
    "print(f\"   RMSE: {metrics['rmse']:.4f}\")\n",
    "print(f\"   MAE: {metrics['mae']:.4f}\")\n",
    "print(f\"   MAPE: {metrics['mape']:.2f}%\")\n",
    "print(f\"   Explained Variance: {metrics['explained_variance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training plots\n",
    "create_training_plots(history, best_epoch, config['save_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation plots\n",
    "create_evaluation_plots(y_test, predictions, config['save_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training history\n",
    "history_df = pd.DataFrame(history)\n",
    "print(\"üìà Training History (last 10 epochs):\")\n",
    "print(history_df.tail(10).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted': predictions,\n",
    "    'residuals': y_test - predictions,\n",
    "    'absolute_error': np.abs(y_test - predictions)\n",
    "})\n",
    "\n",
    "print(\"üîç Prediction Analysis:\")\n",
    "print(predictions_df.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution analysis\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(predictions_df['residuals'], bins=30, alpha=0.7, color='blue')\n",
    "plt.title('Residuals Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(predictions_df['absolute_error'], bins=30, alpha=0.7, color='red')\n",
    "plt.title('Absolute Error Distribution')\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(predictions_df['actual'], predictions_df['absolute_error'], alpha=0.6)\n",
    "plt.title('Error vs Actual Values')\n",
    "plt.xlabel('Actual Bike Count')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "save_results(\n",
    "    model, history, metrics, predictions, y_test, feature_names, \n",
    "    training_time, total_params, config['save_dir'], logger\n",
    ")\n",
    "\n",
    "print(\"\\nüíæ All results saved successfully!\")\n",
    "print(f\"üìÅ Check the '{config['save_dir']}' directory for:\")\n",
    "print(\"   - Training history CSV\")\n",
    "print(\"   - Evaluation metrics CSV\")\n",
    "print(\"   - Predictions CSV\")\n",
    "print(\"   - Model checkpoints (.pth and .pkl)\")\n",
    "print(\"   - Training and evaluation plots\")\n",
    "print(\"   - Complete training log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"üéâ SAINT Training Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Model Performance:\")\n",
    "print(f\"   R¬≤ Score: {metrics['r2_score']:.4f}\")\n",
    "print(f\"   RMSE: {metrics['rmse']:.4f}\")\n",
    "print(f\"   MAE: {metrics['mae']:.4f}\")\n",
    "print(f\"   MAPE: {metrics['mape']:.2f}%\")\n",
    "print(f\"\\n‚öôÔ∏è Model Configuration:\")\n",
    "print(f\"   Architecture: SAINT Transformer\")\n",
    "print(f\"   Parameters: {total_params:,}\")\n",
    "print(f\"   Training time: {training_time:.2f} seconds\")\n",
    "print(f\"   Best epoch: {best_epoch + 1}\")\n",
    "print(f\"\\nüöÄ Model ready for deployment and comparison!\")\n",
    "\n",
    "logger.info(\"SAINT training notebook completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Quick Model Loading Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model loading from saved checkpoint\n",
    "import pickle\n",
    "\n",
    "print(\"üîÑ Testing model loading from saved checkpoint...\")\n",
    "\n",
    "# Load model data\n",
    "with open(f\"{config['save_dir']}/saint_model.pkl\", 'rb') as f:\n",
    "    saved_model_data = pickle.load(f)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Saved metrics: R¬≤ = {saved_model_data['metrics']['r2_score']:.4f}\")\n",
    "print(f\"   Model architecture: {saved_model_data['model_architecture']}\")\n",
    "print(f\"   Feature names: {len(saved_model_data['feature_names'])} features\")\n",
    "print(f\"   Training time: {saved_model_data['training_time']:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
