{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FT-Transformer Training for Airbnb Regression\n",
    "\n",
    "This notebook demonstrates how to train an FT-Transformer model for Airbnb price prediction using the modular training functions.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The FT-Transformer (Feature Tokenizer + Transformer) is a state-of-the-art architecture for tabular data that:\n",
    "- Converts features into embeddings using feature tokenization\n",
    "- Uses multi-head attention to capture feature interactions\n",
    "- Applies layer normalization and residual connections\n",
    "- Provides excellent performance on regression tasks\n",
    "\n",
    "## Dataset\n",
    "- **Source**: Dgomonov's New York City Airbnb Open Data\n",
    "- **Task**: Regression (predicting Airbnb listing prices)\n",
    "- **Features**: Various features including location, property type, reviews, etc.\n",
    "- **Target**: Price per night in USD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all training functions\n",
    "from improved_ft_transformer_training import *\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"ðŸ  FT-Transformer Training for Airbnb Regression\")\n",
    "print(\"Dataset: Airbnb NYC Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data\n",
    "\n",
    "Load the preprocessed Airbnb data from Section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "(X_train_scaled, X_val_scaled, X_test_scaled, \n",
    " y_train, y_val, y_test, feature_names, data_summary) = load_and_analyze_data('./Section1_Data_PreProcessing/airbnb_preprocessed_data.pkl')\n",
    "\n",
    "print(f\"\\nðŸ“Š Data Summary:\")\n",
    "print(f\"   Training samples: {len(X_train_scaled):,}\")\n",
    "print(f\"   Validation samples: {len(X_val_scaled):,}\")\n",
    "print(f\"   Test samples: {len(X_test_scaled):,}\")\n",
    "print(f\"   Features: {len(feature_names)}\")\n",
    "print(f\"   Target range: [{y_train.min():.2f}, {y_train.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Target Scaling\n",
    "\n",
    "Apply robust scaling to the target variable for better training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply target scaling\n",
    "y_train_scaled, y_val_scaled, y_test_scaled, target_scaler = apply_target_scaling(\n",
    "    y_train, y_val, y_test)\n",
    "\n",
    "print(f\"\\nâœ… Target scaling completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Training\n",
    "\n",
    "Convert data to PyTorch tensors and create data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "batch_size = 128\n",
    "\n",
    "(train_loader, val_loader, test_loader, feature_info,\n",
    " X_train_tensor, X_val_tensor, X_test_tensor,\n",
    " y_train_tensor, y_val_tensor, y_test_tensor) = prepare_improved_data(\n",
    "    X_train_scaled, X_val_scaled, X_test_scaled, \n",
    "    y_train_scaled, y_val_scaled, y_test_scaled, \n",
    "    feature_names, device, batch_size)\n",
    "\n",
    "print(f\"\\nâœ… Data preparation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create FT-Transformer Model\n",
    "\n",
    "Create the improved FT-Transformer model for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FT-Transformer model\n",
    "model, total_params = create_improved_ft_transformer(feature_info, device)\n",
    "\n",
    "print(f\"\\nðŸ¤– Model created with {total_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Training Components\n",
    "\n",
    "Setup loss function, optimizer, and scheduler with improved settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training components\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "criterion, optimizer, scheduler, training_config = setup_improved_training(\n",
    "    model, learning_rate, weight_decay)\n",
    "\n",
    "print(f\"\\nðŸ”§ Training setup completed!\")\n",
    "print(f\"   Learning rate: {learning_rate}\")\n",
    "print(f\"   Weight decay: {weight_decay}\")\n",
    "print(f\"   Max epochs: {training_config['n_epochs']}\")\n",
    "print(f\"   Early stopping patience: {training_config['patience']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "\n",
    "Train the FT-Transformer model with improved training loop and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model, history, best_epoch, training_time = train_improved_ft_transformer(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "    training_config, device, target_scaler)\n",
    "\n",
    "print(f\"\\nðŸ Training completed in {training_time:.2f} seconds\")\n",
    "print(f\"   Best epoch: {best_epoch + 1}\")\n",
    "print(f\"   Final validation RÂ²: {history['val_r2'][best_epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Model\n",
    "\n",
    "Evaluate the trained model on the test set with proper unscaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "predictions, metrics, y_test_unscaled = evaluate_improved_model(\n",
    "    model, X_test_tensor, y_test_tensor, device, target_scaler)\n",
    "\n",
    "print(f\"\\nðŸ“Š Test Set Performance:\")\n",
    "print(f\"   RÂ² Score: {metrics['r2_score']:.4f}\")\n",
    "print(f\"   RMSE: {metrics['rmse']:.4f}\")\n",
    "print(f\"   MAE: {metrics['mae']:.4f}\")\n",
    "print(f\"   MAPE: {metrics['mape']:.2f}%\")\n",
    "print(f\"   Explained Variance: {metrics['explained_variance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Visualizations\n",
    "\n",
    "Create training and evaluation plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create improved plots\n",
    "create_improved_plots(history, best_epoch, predictions, y_test_unscaled, metrics, './Section2_Model_Training')\n",
    "\n",
    "print(\"\\nðŸ“ˆ Training and evaluation plots created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results\n",
    "\n",
    "Save all results, model, and generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "save_improved_results(model, history, metrics, predictions, y_test_unscaled, \n",
    "                     feature_names, training_time, total_params, target_scaler, './Section2_Model_Training')\n",
    "\n",
    "print(\"\\nðŸ’¾ All results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Alternative: Run Complete Pipeline\n",
    "\n",
    "Alternatively, you can run the complete pipeline with a single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete pipeline (alternative approach)\n",
    "# Uncomment the following lines to run the complete pipeline in one go:\n",
    "\n",
    "# model, history, metrics, predictions, feature_names, target_scaler = run_improved_ft_transformer_training(\n",
    "#     data_path='./Section1_Data_PreProcessing/airbnb_preprocessed_data.pkl',\n",
    "#     device=device,\n",
    "#     batch_size=128,\n",
    "#     learning_rate=5e-4,\n",
    "#     weight_decay=1e-4,\n",
    "#     save_dir='./Section2_Model_Training'\n",
    "# )\n",
    "\n",
    "print(\"\\nðŸš€ Complete pipeline function available for one-step execution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Analysis and Insights\n",
    "\n",
    "Analyze the trained model performance and provide insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FT-TRANSFORMER PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Model Performance:\")\n",
    "r2_score = metrics['r2_score']\n",
    "if r2_score > 0.9:\n",
    "    performance_level = \"Excellent\"\n",
    "elif r2_score > 0.8:\n",
    "    performance_level = \"Good\"\n",
    "elif r2_score > 0.7:\n",
    "    performance_level = \"Moderate\"\n",
    "else:\n",
    "    performance_level = \"Needs Improvement\"\n",
    "\n",
    "print(f\"   Performance Level: {performance_level} (RÂ² = {r2_score:.4f})\")\n",
    "print(f\"   RMSE: ${metrics['rmse']:.2f}\")\n",
    "print(f\"   MAE: ${metrics['mae']:.2f}\")\n",
    "print(f\"   MAPE: {metrics['mape']:.2f}%\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Characteristics:\")\n",
    "print(f\"   Total Parameters: {total_params:,}\")\n",
    "print(f\"   Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"   Best Epoch: {best_epoch + 1}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Business Insights:\")\n",
    "avg_actual = y_test_unscaled.mean()\n",
    "avg_error = metrics['mae']\n",
    "error_percentage = (avg_error / avg_actual) * 100\n",
    "\n",
    "print(f\"   Average Airbnb price: ${avg_actual:.2f}\")\n",
    "print(f\"   Average prediction error: ${avg_error:.2f} ({error_percentage:.1f}%)\")\n",
    "\n",
    "if error_percentage < 10:\n",
    "    print(f\"   âœ… Excellent accuracy for pricing decisions\")\n",
    "elif error_percentage < 20:\n",
    "    print(f\"   âœ… Good accuracy for market analysis\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ Consider model improvements for better accuracy\")\n",
    "\n",
    "print(f\"\\nðŸ“ Generated Files:\")\n",
    "print(f\"   - Training History: ./Section2_Model_Training/improved_ft_transformer_training_history.csv\")\n",
    "print(f\"   - Evaluation Metrics: ./Section2_Model_Training/improved_ft_transformer_evaluation_metrics.csv\")\n",
    "print(f\"   - Predictions: ./Section2_Model_Training/improved_ft_transformer_predictions.csv\")\n",
    "print(f\"   - Model Checkpoint: ./Section2_Model_Training/improved_ft_transformer_model.pth\")\n",
    "print(f\"   - Training Plots: ./Section2_Model_Training/Improved_FT_Transformer_results.png\")\n",
    "\n",
    "print(f\"\\nðŸš€ FT-Transformer training completed successfully!\")\n",
    "print(f\"   Model ready for deployment and comparison with other models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. **Load preprocessed data** from the Airbnb dataset\n",
    "2. **Apply target scaling** for improved training stability\n",
    "3. **Prepare data** for FT-Transformer training with PyTorch tensors\n",
    "4. **Create an improved FT-Transformer model** specifically for regression\n",
    "5. **Train the model** with enhanced training loop and early stopping\n",
    "6. **Evaluate performance** using comprehensive regression metrics\n",
    "7. **Generate visualizations** for training progress and model performance\n",
    "8. **Save all results** for future analysis and comparison\n",
    "\n",
    "The improved FT-Transformer provides state-of-the-art performance on tabular data by leveraging attention mechanisms to capture complex feature interactions, making it particularly effective for Airbnb price prediction.\n",
    "\n",
    "### Key Features:\n",
    "- **Improved architecture**: Enhanced with regularization and better hyperparameters\n",
    "- **Target scaling**: RobustScaler for better handling of outliers\n",
    "- **Enhanced training**: Gradient clipping, learning rate warmup, and better monitoring\n",
    "- **Comprehensive evaluation**: Multiple regression metrics and visualizations\n",
    "- **Reproducible results**: Fixed random seeds and saved model checkpoints\n",
    "- **GPU support**: Automatic device detection and memory management\n",
    "\n",
    "### Next Steps:\n",
    "- Compare with other models (XGBoost, SAINT, etc.)\n",
    "- Perform hyperparameter tuning\n",
    "- Analyze feature importance and model interpretability\n",
    "- Deploy the model for real-time price predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
